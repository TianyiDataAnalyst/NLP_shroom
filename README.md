# Multilingual Hallucination Detection - Mu-SHROOM 2025

[![GitHub](https://img.shields.io/badge/GitHub-Repository-blue?logo=github)](https://github.com/TianyiDataAnalyst/NLP_shroom)
[![Python](https://img.shields.io/badge/Python-3.8+-blue?logo=python)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red?logo=pytorch)](https://pytorch.org)
[![Transformers](https://img.shields.io/badge/🤗-Transformers-yellow)](https://huggingface.co/transformers)

A comprehensive NLP project implementing advanced multilingual hallucination detection using XLM-RoBERTa models for the SemEval-2025 Task-3 Mu-SHROOM shared task.

## 🎯 Problem Definition

This project addresses the critical challenge of **multilingual hallucination detection** in large language model outputs. The task involves identifying spans of text that contain factually incorrect or unsupported information while being grammatically sound and contextually plausible.

### **Core Challenge**
- **Input**: Question-answer pairs generated by various LLMs across 10 languages
- **Output**: Precise identification of hallucinated text spans at token and span levels
- **Complexity**: Cross-lingual transfer learning with limited training data and severe class imbalance

### **Real-world Impact**
Hallucination detection is crucial for:
- **AI Safety**: Preventing misinformation in AI-generated content
- **Content Verification**: Automated fact-checking systems
- **Multilingual Applications**: Ensuring reliability across diverse languages
- **Trust in AI**: Building confidence in LLM-generated outputs

## 📊 Dataset Description

### **Mu-SHROOM Dataset (SemEval-2025 Task-3)**
- **Source**: [Helsinki NLP - Mu-SHROOM](https://helsinki-nlp.github.io/shroom/)
- **Task Type**: Multilingual sequence labeling for hallucination detection
- **Format**: JSONL files with token-level and span-level annotations

### **Dataset Characteristics**
```
Languages: 10 (Arabic, Chinese, English, Finnish, French, German, Hindi, Italian, Spanish, Swedish)
Training Samples: 449 samples across 9 languages (English excluded for testing)
Validation Samples: 50 samples per language
Test Samples: 50 samples (English)
Total Tokens: ~17,500 tokens in test set
Hallucination Rate: ~3.6% of tokens (severe class imbalance)
```

### **Data Structure**
```json
{
  "id": "val-en-1",
  "lang": "EN", 
  "model_input": "What did Petra van Staveren win a gold medal for?",
  "model_output_text": "Petra van Stoveren won a silver medal in the 2008 Summer Olympics...",
  "model_id": "tiiuae/falcon-7b-instruct",
  "soft_labels": [{"start": 25, "prob": 0.9, "end": 31}],
  "hard_labels": [[25, 31], [45, 49]],
  "model_output_tokens": ["Pet", "ra", "Ġvan", "ĠSto", "ve", "ren", ...],
  "model_output_logits": [-5.57, -11.91, -13.07, ...]
}
```

### **Annotation Details**
- **Hard Labels**: Binary span boundaries for definitive hallucinations
- **Soft Labels**: Probabilistic annotations with confidence scores
- **Token-level**: Individual token classifications
- **Span-level**: Contiguous hallucinated text segments

## 📏 Evaluation Metrics

### **Primary Metrics**
- **Span-level F1 Score**: Primary evaluation metric measuring exact boundary detection
- **Token-level F1 Score**: Secondary metric for individual token classification
- **Precision & Recall**: Both span-level and token-level granularity

### **Metric Definitions**
```python
# Span-level: Exact boundary matching required
Span F1 = 2 * (Span Precision * Span Recall) / (Span Precision + Span Recall)

# Token-level: Individual token classification
Token F1 = 2 * (Token Precision * Token Recall) / (Token Precision + Token Recall)

# Overall Accuracy: Percentage of correctly classified tokens
Accuracy = (True Positives + True Negatives) / Total Tokens
```

### **Evaluation Challenges**
- **Strict Span Matching**: Requires exact start and end boundary detection
- **Class Imbalance**: Only ~3.6% of tokens are hallucinations
- **Cross-lingual Transfer**: Model trained on 9 languages, tested on English
- **Boundary Sensitivity**: Small boundary errors severely impact span-level scores

## 🏗️ Model Architecture

### **Base Model: XLM-RoBERTa**
```
Architecture: XLM-RoBERTa Base & Large
Parameters: 278M (Base) / 550M (Large)
Hidden Size: 768 (Base) / 1024 (Large)
Attention Heads: 12 (Base) / 16 (Large)
Hidden Layers: 12 (Base) / 24 (Large)
Max Sequence Length: 512
Vocabulary Size: 250,002
Multilingual Support: 100+ languages
```

### **Training Strategy**
- **Leave-One-Language-Out**: Train on 9 languages, test on English
- **Token Classification**: BIO tagging scheme for sequence labeling
- **Cross-lingual Transfer**: Leverage multilingual representations
- **Fine-tuning**: Task-specific adaptation of pre-trained models

### **Advanced Techniques**
- **Enhanced Training**: Extended epochs (2→12) with early stopping
- **Optimization**: Warmup scheduling, gradient accumulation, mixed precision
- **Ensemble Methods**: Majority voting and logit averaging
- **Data Augmentation**: Planned synthetic data generation

## 📈 Results and Performance

### **Baseline Performance**
```
Model: XLM-RoBERTa Base (2 epochs)
Training Time: 1.75 minutes
Training Samples: 449 (9 languages)

Results on English Test Set (50 samples):
├── Span-level F1: 9.49%
├── Token-level F1: 32.01%
├── Overall Accuracy: 95.99%
├── Span Precision: 7.77%
├── Span Recall: 12.20%
├── Token Precision: 41.25%
└── Token Recall: 26.15%
```

### **Advanced Model Performance**
```
Model: XLM-RoBERTa Base (12 epochs, enhanced training)
Training Time: 22.58 minutes
Training Samples: 336 (75% split)

Results on English Test Set:
├── Span-level F1: 8.38% (-11.8% vs baseline)
├── Token-level F1: 42.66% (+33.3% vs baseline) ⭐
├── Overall Accuracy: 93.95%
├── Span Precision: 6.27%
├── Span Recall: 12.60%
├── Token Precision: 32.75%
└── Token Recall: 61.18% (+134% vs baseline) ⭐
```

### **Key Improvements**
- **Token-level F1**: 32.01% → 42.66% (**+33.3% improvement**)
- **Token Recall**: 26.15% → 61.18% (**+134% improvement**)
- **Training Optimization**: Proper convergence with 12 epochs vs 2
- **Early Stopping**: Prevented overfitting at epoch 11

### **Performance Analysis**
- **Strengths**: Significant improvement in token-level detection and recall
- **Challenges**: Span boundary detection remains difficult
- **Trade-offs**: Higher recall achieved at cost of precision
- **Bottlenecks**: Limited training data (336 samples) and severe class imbalance

## 🚀 Quick Start

### **Installation**
```bash
# Clone repository
git clone https://github.com/TianyiDataAnalyst/NLP_shroom.git
cd NLP_shroom

# Install dependencies
pip install torch>=2.0.0 transformers>=4.30.0 datasets>=2.0.0
pip install evaluate seqeval matplotlib seaborn scikit-learn
```

### **Basic Usage**
```bash
# Train baseline model
python baseline_training_validation.py --test_lang en --epochs 2

# Train advanced model
python advanced_model_experiments.py --test_lang en --epochs 12

# Evaluate models
python advanced_model_evaluation.py

# Run ensemble methods
python ensemble_methods.py
```

### **Hardware Requirements**
- **GPU**: CUDA-capable with 8GB+ memory (16GB+ for Large models)
- **RAM**: 16GB+ system memory
- **Storage**: 10GB+ for model checkpoints

## 📁 Repository Structure

```
NLP_shroom/
├── README.md                              # This comprehensive guide
├── NLP_project_documentation.md           # Detailed project documentation
├── baseline_performance_report.md         # Baseline model analysis
├── section7_advanced_experiments_report.md # Advanced experiments report
├── SECTION7_README.md                     # Section 7 implementation guide
├── 
├── Core Implementation/
│   ├── baseline_training_validation.py    # Baseline model training
│   ├── baseline_inference_evaluation.py   # Baseline evaluation
│   ├── advanced_model_experiments.py      # Enhanced training framework
│   ├── advanced_model_evaluation.py       # Advanced evaluation system
│   └── ensemble_methods.py               # Multi-model ensemble methods
├── 
├── Results & Checkpoints/
│   ├── advanced_results/                  # Advanced model checkpoints
│   │   └── xlm-roberta-base_epochs_12/   # 12-epoch trained model
│   ├── advanced_evaluation/              # Advanced evaluation results
│   └── ensemble_results/                 # Ensemble method results
└── 
└── Documentation/
    ├── .gitignore                        # Git ignore configuration
    └── [Additional documentation files]
```

## 🎯 Future Work

### **Planned Improvements**
- **Data Augmentation**: Synthetic hallucination generation and back-translation
- **Advanced Architectures**: CRF layers and BiLSTM-CRF for better sequence labeling
- **Hyperparameter Optimization**: Systematic grid search and Bayesian optimization
- **Cross-validation**: K-fold validation for robust evaluation

### **Expected Targets**
- **Span-level F1**: 8.38% → 15-25% (with data augmentation)
- **Token-level F1**: 42.66% → 55-70% (with advanced architectures)
- **Cross-lingual Consistency**: Evaluate performance across all 10 languages

## 📚 References

- **Mu-SHROOM Task**: [SemEval-2025 Task-3](https://helsinki-nlp.github.io/shroom/)
- **XLM-RoBERTa**: [Conneau et al., 2020](https://huggingface.co/FacebookAI/xlm-roberta-base)
- **Transformers Library**: [Hugging Face](https://huggingface.co/docs/transformers)
- **Original SHROOM**: [SemEval-2024 Task-6](https://helsinki-nlp.github.io/shroom/)

## 📄 License

This project is part of an academic NLP course submission. Please refer to the original Mu-SHROOM task guidelines for data usage terms.

## 🤝 Contributing

This is an academic project. For questions or collaboration:
- **Repository**: [GitHub - NLP_shroom](https://github.com/TianyiDataAnalyst/NLP_shroom)
- **Branch**: `section7-advanced-experiments`
- **Documentation**: See `NLP_project_documentation.md` for detailed project information

---

**Project Status**: ✅ **Section 7 Complete** - Advanced experiments with significant performance improvements achieved  
**Last Updated**: July 2025  
**Key Achievement**: +33.3% Token F1 improvement through enhanced training techniques
